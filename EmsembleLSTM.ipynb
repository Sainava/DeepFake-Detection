{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b04c1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loaded features: torch.Size([2000, 32, 2048])\n",
      "Loaded labels: torch.Size([2000])\n",
      "model1_lstm Epoch 1 | Loss: 0.5515 | Val Acc: 0.8300\n",
      "model1_lstm Epoch 2 | Loss: 0.5349 | Val Acc: 0.4833\n",
      "model1_lstm Epoch 3 | Loss: 0.5384 | Val Acc: 0.8333\n",
      "model1_lstm Epoch 4 | Loss: 0.5063 | Val Acc: 0.8433\n",
      "model1_lstm Epoch 5 | Loss: 0.5094 | Val Acc: 0.8400\n",
      "model1_lstm Epoch 6 | Loss: 0.4878 | Val Acc: 0.8300\n",
      "model1_lstm Epoch 7 | Loss: 0.4948 | Val Acc: 0.8433\n",
      "model1_lstm Epoch 8 | Loss: 0.4663 | Val Acc: 0.8233\n",
      "model1_lstm Epoch 9 | Loss: 0.4759 | Val Acc: 0.8433\n",
      "model1_lstm Epoch 10 | Loss: 0.4618 | Val Acc: 0.8500\n",
      "model1_lstm Epoch 11 | Loss: 0.4599 | Val Acc: 0.8467\n",
      "model1_lstm Epoch 12 | Loss: 0.4562 | Val Acc: 0.8433\n",
      "model1_lstm Epoch 13 | Loss: 0.4754 | Val Acc: 0.8433\n",
      "model1_lstm Epoch 14 | Loss: 0.4568 | Val Acc: 0.8300\n",
      "model1_lstm Epoch 15 | Loss: 0.4657 | Val Acc: 0.8367\n",
      "model1_lstm Epoch 16 | Loss: 0.4796 | Val Acc: 0.6367\n",
      "model1_lstm Epoch 17 | Loss: 0.4865 | Val Acc: 0.8300\n",
      "model1_lstm Epoch 18 | Loss: 0.4840 | Val Acc: 0.8367\n",
      "model1_lstm Epoch 19 | Loss: 0.4681 | Val Acc: 0.4933\n",
      "model1_lstm Epoch 20 | Loss: 0.4863 | Val Acc: 0.8367\n",
      "model1_lstm Epoch 21 | Loss: 0.4885 | Val Acc: 0.8400\n",
      "model1_lstm Epoch 22 | Loss: 0.4727 | Val Acc: 0.8167\n",
      "model1_lstm Epoch 23 | Loss: 0.4747 | Val Acc: 0.8400\n",
      "model1_lstm Epoch 24 | Loss: 0.4679 | Val Acc: 0.7367\n",
      "model1_lstm Epoch 25 | Loss: 0.4602 | Val Acc: 0.8200\n",
      "model1_lstm Epoch 26 | Loss: 0.4530 | Val Acc: 0.8233\n",
      "model1_lstm Epoch 27 | Loss: 0.4423 | Val Acc: 0.8367\n",
      "model1_lstm Epoch 28 | Loss: 0.4560 | Val Acc: 0.8467\n",
      "model1_lstm Epoch 29 | Loss: 0.4460 | Val Acc: 0.8533\n",
      "model1_lstm Epoch 30 | Loss: 0.4355 | Val Acc: 0.8533\n",
      "âœ… Saved best model1_lstm model with Val Acc: 0.8533\n",
      "\n",
      "model2_lstm Epoch 1 | Loss: 0.5402 | Val Acc: 0.8200\n",
      "model2_lstm Epoch 2 | Loss: 0.5124 | Val Acc: 0.8200\n",
      "model2_lstm Epoch 3 | Loss: 0.5261 | Val Acc: 0.8400\n",
      "model2_lstm Epoch 4 | Loss: 0.4975 | Val Acc: 0.8367\n",
      "model2_lstm Epoch 5 | Loss: 0.5045 | Val Acc: 0.8267\n",
      "model2_lstm Epoch 6 | Loss: 0.4872 | Val Acc: 0.8367\n",
      "model2_lstm Epoch 7 | Loss: 0.4816 | Val Acc: 0.8367\n",
      "model2_lstm Epoch 8 | Loss: 0.4680 | Val Acc: 0.8333\n",
      "model2_lstm Epoch 9 | Loss: 0.4711 | Val Acc: 0.8433\n",
      "model2_lstm Epoch 10 | Loss: 0.4581 | Val Acc: 0.8467\n",
      "model2_lstm Epoch 11 | Loss: 0.4400 | Val Acc: 0.8467\n",
      "model2_lstm Epoch 12 | Loss: 0.4503 | Val Acc: 0.8467\n",
      "model2_lstm Epoch 13 | Loss: 0.4755 | Val Acc: 0.8367\n",
      "model2_lstm Epoch 14 | Loss: 0.4784 | Val Acc: 0.8333\n",
      "model2_lstm Epoch 15 | Loss: 0.4716 | Val Acc: 0.8433\n",
      "model2_lstm Epoch 16 | Loss: 0.4762 | Val Acc: 0.8233\n",
      "model2_lstm Epoch 17 | Loss: 0.4577 | Val Acc: 0.8467\n",
      "model2_lstm Epoch 18 | Loss: 0.4851 | Val Acc: 0.8500\n",
      "model2_lstm Epoch 19 | Loss: 0.4842 | Val Acc: 0.8233\n",
      "model2_lstm Epoch 20 | Loss: 0.4782 | Val Acc: 0.8467\n",
      "model2_lstm Epoch 21 | Loss: 0.4594 | Val Acc: 0.8367\n",
      "model2_lstm Epoch 22 | Loss: 0.4758 | Val Acc: 0.8533\n",
      "model2_lstm Epoch 23 | Loss: 0.4575 | Val Acc: 0.8267\n",
      "model2_lstm Epoch 24 | Loss: 0.4832 | Val Acc: 0.8433\n",
      "model2_lstm Epoch 25 | Loss: 0.4502 | Val Acc: 0.8400\n",
      "model2_lstm Epoch 26 | Loss: 0.4586 | Val Acc: 0.8400\n",
      "model2_lstm Epoch 27 | Loss: 0.4496 | Val Acc: 0.8433\n",
      "model2_lstm Epoch 28 | Loss: 0.4527 | Val Acc: 0.8467\n",
      "model2_lstm Epoch 29 | Loss: 0.4270 | Val Acc: 0.8500\n",
      "model2_lstm Epoch 30 | Loss: 0.4410 | Val Acc: 0.8500\n",
      "âœ… Saved best model2_lstm model with Val Acc: 0.8533\n",
      "\n",
      "model3_gru Epoch 1 | Loss: 0.5419 | Val Acc: 0.8267\n",
      "model3_gru Epoch 2 | Loss: 0.5493 | Val Acc: 0.8033\n",
      "model3_gru Epoch 3 | Loss: 0.5079 | Val Acc: 0.4900\n",
      "model3_gru Epoch 4 | Loss: 0.5109 | Val Acc: 0.8367\n",
      "model3_gru Epoch 5 | Loss: 0.4835 | Val Acc: 0.4833\n",
      "model3_gru Epoch 6 | Loss: 0.4835 | Val Acc: 0.8200\n",
      "model3_gru Epoch 7 | Loss: 0.4908 | Val Acc: 0.8433\n",
      "model3_gru Epoch 8 | Loss: 0.4832 | Val Acc: 0.8467\n",
      "model3_gru Epoch 9 | Loss: 0.4612 | Val Acc: 0.8333\n",
      "model3_gru Epoch 10 | Loss: 0.4705 | Val Acc: 0.8433\n",
      "model3_gru Epoch 11 | Loss: 0.4544 | Val Acc: 0.8433\n",
      "model3_gru Epoch 12 | Loss: 0.4764 | Val Acc: 0.8333\n",
      "model3_gru Epoch 13 | Loss: 0.4698 | Val Acc: 0.8533\n",
      "model3_gru Epoch 14 | Loss: 0.4642 | Val Acc: 0.8500\n",
      "model3_gru Epoch 15 | Loss: 0.4806 | Val Acc: 0.8533\n",
      "model3_gru Epoch 16 | Loss: 0.4949 | Val Acc: 0.8367\n",
      "model3_gru Epoch 17 | Loss: 0.4667 | Val Acc: 0.8333\n",
      "model3_gru Epoch 18 | Loss: 0.4995 | Val Acc: 0.8367\n",
      "model3_gru Epoch 19 | Loss: 0.4911 | Val Acc: 0.5333\n",
      "model3_gru Epoch 20 | Loss: 0.4869 | Val Acc: 0.7200\n",
      "model3_gru Epoch 21 | Loss: 0.4999 | Val Acc: 0.8333\n",
      "model3_gru Epoch 22 | Loss: 0.4799 | Val Acc: 0.8300\n",
      "model3_gru Epoch 23 | Loss: 0.4848 | Val Acc: 0.7433\n",
      "model3_gru Epoch 24 | Loss: 0.4925 | Val Acc: 0.8233\n",
      "model3_gru Epoch 25 | Loss: 0.4856 | Val Acc: 0.8467\n",
      "model3_gru Epoch 26 | Loss: 0.4706 | Val Acc: 0.8500\n",
      "model3_gru Epoch 27 | Loss: 0.4577 | Val Acc: 0.8600\n",
      "model3_gru Epoch 28 | Loss: 0.4670 | Val Acc: 0.8467\n",
      "model3_gru Epoch 29 | Loss: 0.4623 | Val Acc: 0.8667\n",
      "model3_gru Epoch 30 | Loss: 0.4436 | Val Acc: 0.8567\n",
      "âœ… Saved best model3_gru model with Val Acc: 0.8667\n",
      "\n",
      "\n",
      "ðŸŽ¯ Ensemble Test Accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------ Load Data ------------------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "features = torch.load(\"all_features.pt\")  # [N, T, 2048]\n",
    "labels = torch.load(\"all_labels.pt\")      # [N]\n",
    "\n",
    "print(\"Loaded features:\", features.shape)\n",
    "print(\"Loaded labels:\", labels.shape)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "dataset = TensorDataset(features, labels)\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# ------------------ Models ------------------\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        scores = self.attn(lstm_out)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = (weights * lstm_out).sum(dim=1)\n",
    "        return context\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(2048, hidden_dim, num_layers=2, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.attn_pool = AttentionPool(hidden_dim)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden_dim // 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_vec = self.attn_pool(lstm_out)\n",
    "        attn_vec = self.batchnorm(attn_vec)\n",
    "        return self.fc(attn_vec)\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(2048, hidden_dim, num_layers=2, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.attn_pool = AttentionPool(hidden_dim)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden_dim // 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        attn_vec = self.attn_pool(gru_out)\n",
    "        attn_vec = self.batchnorm(attn_vec)\n",
    "        return self.fc(attn_vec)\n",
    "\n",
    "\n",
    "# ------------------ Train Function ------------------\n",
    "import random\n",
    "\n",
    "# ------------------ Seed Control ------------------\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ------------------ Train Function ------------------\n",
    "def train_model(model, model_name, seed=42):\n",
    "    set_seed(seed)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(30):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # âœ… Gradient clipping\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += y_val.size(0)\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "\n",
    "        acc = correct / total\n",
    "        scheduler.step()\n",
    "        print(f\"{model_name} Epoch {epoch+1} | Loss: {avg_loss:.4f} | Val Acc: {acc:.4f}\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    torch.save(best_state, f\"{model_name}.pth\")\n",
    "    print(f\"âœ… Saved best {model_name} model with Val Acc: {best_acc:.4f}\\n\")\n",
    "\n",
    "\n",
    "# ------------------ Train All Models ------------------\n",
    "train_model(LSTMClassifier(hidden_dim=768, dropout=0.5), \"model1_lstm\", seed=42)\n",
    "train_model(LSTMClassifier(hidden_dim=512, dropout=0.3), \"model2_lstm\", seed=17)\n",
    "train_model(GRUClassifier(hidden_dim=768, dropout=0.4), \"model3_gru\", seed=77)\n",
    "\n",
    "\n",
    "# ------------------ Ensemble Prediction ------------------\n",
    "def ensemble_predict(models, x):\n",
    "    probs = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "            prob = F.softmax(out, dim=1)\n",
    "            probs.append(prob)\n",
    "    avg_prob = torch.stack(probs).mean(dim=0)\n",
    "    return avg_prob.argmax(dim=1)\n",
    "\n",
    "# ------------------ Load Models for Inference ------------------\n",
    "model1 = LSTMClassifier(hidden_dim=768, dropout=0.5)\n",
    "model2 = LSTMClassifier(hidden_dim=512, dropout=0.3)\n",
    "model3 = GRUClassifier(hidden_dim=768, dropout=0.4)\n",
    "model1.load_state_dict(torch.load(\"model1_lstm.pth\"))\n",
    "model2.load_state_dict(torch.load(\"model2_lstm.pth\"))\n",
    "model3.load_state_dict(torch.load(\"model3_gru.pth\"))\n",
    "\n",
    "models = [model1.to(device), model2.to(device), model3.to(device)]\n",
    "\n",
    "# ------------------ Final Evaluation ------------------\n",
    "correct = 0\n",
    "total = 0\n",
    "for x_batch, y_batch in test_loader:\n",
    "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "    preds = ensemble_predict(models, x_batch)\n",
    "    correct += (preds == y_batch).sum().item()\n",
    "    total += y_batch.size(0)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Ensemble Test Accuracy: {correct / total:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
